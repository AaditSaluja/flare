{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Merge multiple GP models\n",
    "\n",
    "There are situations where we train several GP models separately, and need to merge them into one model. Here is the instruction. \n",
    "\n",
    "Suppose we have two GP models from files `gp1` and `gp2`, we need to \n",
    "\n",
    "Step 1: Concatenate the training data of the two models together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flare.gp import GaussianProcess\n",
    "\n",
    "# load GP models from file\n",
    "gp1 = GaussianProcess.from_file(\"gp_model_1.json\")\n",
    "gp2 = GaussianProcess.from_file(\"gp_model_2.json\")\n",
    "\n",
    "# append the training data set of gp2 to that of gp1\n",
    "gp_tot = gp1\n",
    "gp_tot.training_data += gp2.training_data\n",
    "gp_tot.training_labels += gp2.training_labels\n",
    "gp_tot.training_labels_np = np.array(gp_tot.training_labels)\n",
    "\n",
    "# Optional: if you've also included total energy label into the training set\n",
    "gp_tot.training_structures += gp2.training_structures\n",
    "gp_tot.energy_labels += gp2.energy_labels\n",
    "gp_tot.energy_labels_np = np.array(gp_tot.energy_labels_np)\n",
    "\n",
    "# sync data\n",
    "gp_tot.all_labels = np.concatenate((gp_tot.training_labels_np, gp_tot.energy_labels_np))\n",
    "gp_tot.sync_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Train the hyperparameters of the merged GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_tot.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2*: If you don't want to train the hyperparameters, you still need to call `set_L_alpha()` once to compute the new kernel matrix. If you called `train()`, then you don't need this step because the kernel matrix is computed during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_tot.set_L_alpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Write model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_tot.write_model(\"gp_model_tot.json\")"
   ]
  }
 ]
}